{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import re\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import random\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import torch\n",
    "from src.peaks.finder import PeakFinder\n",
    "\n",
    "import mlflow\n",
    "from config.loader import load_config\n",
    "import os\n",
    "\n",
    "import src.measurements.api as mpi\n",
    "import src.generator.api as gpi\n",
    "import src.peaks.api as ppi\n",
    "import src.statistics.api as spi\n",
    "plt.rcParams['text.usetex'] = True\n",
    "from src.cnn.training import Training\n",
    "import json\n",
    "from src.cnn.dataset import MeasurementTraining\n",
    "import mlflow\n",
    "import os\n",
    "from config.loader import load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = load_config()[\"minio\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = load_config()[\"minio\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = load_config()[\"minio\"][\"MLFLOW_S3_ENDPOINT_URL\"]\n",
    "model_uri = load_config()[\"mlflow\"][\"uri\"]\n",
    "model_name = \"CNN_CPU\"\n",
    "model_version = \"latest\"\n",
    "mlflow.set_tracking_uri(uri=model_uri)\n",
    "model = mlflow.pytorch.load_model(f\"models:/{model_name}/{model_version}\").to(\"cpu\")\n",
    "\n",
    "client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=load_config()[\"mlflow\"][\"uri\"]\n",
    ")\n",
    "run_id = client.get_latest_versions(\"CNN_CPU\")[0].run_id\n",
    "# MODEL REAL MEASUREMENTS ONLY\n",
    "run_id = \"e78098da07bb482aa6b451bd7c6fc310\"\n",
    "run = client.get_run(run_id)\n",
    "client.download_artifacts(run_id=run_id, path=\"artifacts.json\", dst_path=\"tmp/measurements_only\")\n",
    "mlb_classes = run.data.params[\"mlb_classes\"].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_macro_loss = client.get_metric_history(run_id=run_id, key=\"training_macro_loss\")\n",
    "training_micro_loss = client.get_metric_history(run_id=run_id, key=\"training_micro_loss\")\n",
    "training_mac_loss = []\n",
    "training_mic_loss = []\n",
    "\n",
    "validation_macro_loss = client.get_metric_history(run_id=run_id, key=\"validation_macro_loss\")\n",
    "validation_mac_loss = []\n",
    "validation_mic_loss = []\n",
    "for i in range(len(training_macro_loss)):\n",
    "    macro_loss = training_macro_loss[i].value\n",
    "    training_mac_loss.append(macro_loss)\n",
    "    micro_loss = training_micro_loss[i].value\n",
    "    training_mic_loss.append(micro_loss)\n",
    "\n",
    "    val_macro_loss = validation_macro_loss[i].value\n",
    "    validation_mac_loss.append(val_macro_loss)\n",
    "\n",
    "\n",
    "data_mic_mac_loss = pd.DataFrame([training_mac_loss, training_mic_loss, validation_mac_loss])\n",
    "data_mic_mac_loss = data_mic_mac_loss.T.rename(columns={0:\"training_macro_loss\", 1:\"training_micro_loss\",\n",
    "                                                        2:\"validation_macro_loss\"})\n",
    "data_mic_mac_loss[\"epoch\"] = data_mic_mac_loss.index\n",
    "data_mic_mac_loss[\"type\"] = \"measurements_only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = load_config()[\"minio\"][\"AWS_ACCESS_KEY_ID\"]\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = load_config()[\"minio\"][\"AWS_SECRET_ACCESS_KEY\"]\n",
    "os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = load_config()[\"minio\"][\"MLFLOW_S3_ENDPOINT_URL\"]\n",
    "model_uri = load_config()[\"mlflow\"][\"uri\"]\n",
    "model_name = \"CNN_CPU\"\n",
    "model_version = \"latest\"\n",
    "mlflow.set_tracking_uri(uri=model_uri)\n",
    "model = mlflow.pytorch.load_model(f\"models:/{model_name}/{model_version}\").to(\"cpu\")\n",
    "\n",
    "client = mlflow.tracking.MlflowClient(\n",
    "    tracking_uri=load_config()[\"mlflow\"][\"uri\"]\n",
    ")\n",
    "run_id = client.get_latest_versions(\"CNN_CPU\")[0].run_id\n",
    "# MODEL REAL SYNTHETICS ONLY\n",
    "run_id = \"afbd04ca343d42a29f3d1c8fe7e97b61\"\n",
    "run = client.get_run(run_id)\n",
    "client.download_artifacts(run_id=run_id, path=\"artifacts.json\", dst_path=\"tmp/synthetics_only\")\n",
    "mlb_classes = run.data.params[\"mlb_classes\"].split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_macro_loss = client.get_metric_history(run_id=run_id, key=\"training_macro_loss\")\n",
    "training_micro_loss = client.get_metric_history(run_id=run_id, key=\"training_micro_loss\")\n",
    "training_mac_loss = []\n",
    "training_mic_loss = []\n",
    "\n",
    "validation_macro_loss = client.get_metric_history(run_id=run_id, key=\"validation_macro_loss\")\n",
    "validation_mac_loss = []\n",
    "validation_mic_loss = []\n",
    "for i in range(len(training_macro_loss)):\n",
    "    macro_loss = training_macro_loss[i].value\n",
    "    training_mac_loss.append(macro_loss)\n",
    "    micro_loss = training_micro_loss[i].value\n",
    "    training_mic_loss.append(micro_loss)\n",
    "\n",
    "    val_macro_loss = validation_macro_loss[i].value\n",
    "    validation_mac_loss.append(val_macro_loss)\n",
    "\n",
    "\n",
    "data_mic_mac_loss_syntetics_only = pd.DataFrame([training_mac_loss, training_mic_loss, validation_mac_loss])\n",
    "data_mic_mac_loss_syntetics_only = data_mic_mac_loss_syntetics_only.T.rename(columns={0:\"training_macro_loss\", 1:\"training_micro_loss\",\n",
    "                                                        2:\"validation_macro_loss\"})\n",
    "data_mic_mac_loss_syntetics_only[\"epoch\"] = data_mic_mac_loss_syntetics_only.index\n",
    "data_mic_mac_loss_syntetics_only[\"type\"] = \"synthetics_only\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_mic_mac_loss_all = pd.concat([data_mic_mac_loss, data_mic_mac_loss_syntetics_only], axis=0).reset_index(drop=True)\n",
    "data_mic_mac_loss_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp/measurements_only/artifacts.json\") as f:\n",
    "    artifacts = json.load(f)\n",
    "# dict_keys(['used_keys', 'training_tpr', 'training_fpr', 'training_auc', 'validation_tpr', 'validation_fpr', 'validation_auc'])\n",
    "\n",
    "data_tpr_fpr = pd.DataFrame()\n",
    "\n",
    "for idx in range(len(artifacts[\"validation_tpr\"])):\n",
    "    for nuclide in artifacts[\"validation_tpr\"][idx].keys():\n",
    "        nuclide_df = pd.DataFrame(artifacts[\"validation_tpr\"][idx][nuclide], columns=[\"validation_tpr\"])\n",
    "        nuclide_df[\"validation_fpr\"] = artifacts[\"validation_fpr\"][idx][nuclide]\n",
    "        nuclide_df[\"nuclide\"] = nuclide\n",
    "        nuclide_df[\"epoch\"] = idx\n",
    "        data_tpr_fpr = pd.concat([data_tpr_fpr, nuclide_df], axis=0)\n",
    "data_tpr_fpr = data_tpr_fpr.reset_index(drop=True)\n",
    "data_tpr_fpr[\"type\"] = \"measurements_only\"\n",
    "\n",
    "data_auc = pd.DataFrame()\n",
    "\n",
    "for idx in range(len(artifacts[\"training_auc\"])):\n",
    "    for nuclide in artifacts[\"training_auc\"][idx].keys():\n",
    "        nuclide_df = pd.DataFrame([artifacts[\"training_auc\"][idx][nuclide]], columns=[\"training_auc\"])\n",
    "        nuclide_df[\"validation_auc\"] = artifacts[\"validation_auc\"][idx][nuclide]\n",
    "        nuclide_df[\"nuclide\"] = nuclide\n",
    "        nuclide_df[\"epoch\"] = idx\n",
    "        data_auc = pd.concat([data_auc, nuclide_df], axis=0)\n",
    "data_auc[\"type\"] = \"measurements_only\"\n",
    "data_auc = data_auc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tmp/synthetics_only/artifacts.json\") as f:\n",
    "    artifacts = json.load(f)\n",
    "# dict_keys(['used_keys', 'training_tpr', 'training_fpr', 'training_auc', 'validation_tpr', 'validation_fpr', 'validation_auc'])\n",
    "\n",
    "data_tpr_fpr_synthetics_only = pd.DataFrame()\n",
    "\n",
    "for idx in range(len(artifacts[\"validation_tpr\"])):\n",
    "    for nuclide in artifacts[\"validation_tpr\"][idx].keys():\n",
    "        nuclide_df = pd.DataFrame(artifacts[\"validation_tpr\"][idx][nuclide], columns=[\"validation_tpr\"])\n",
    "        nuclide_df[\"validation_fpr\"] = artifacts[\"validation_fpr\"][idx][nuclide]\n",
    "        nuclide_df[\"nuclide\"] = nuclide\n",
    "        nuclide_df[\"epoch\"] = idx\n",
    "        data_tpr_fpr_synthetics_only = pd.concat([data_tpr_fpr_synthetics_only, nuclide_df], axis=0)\n",
    "data_tpr_fpr_synthetics_only = data_tpr_fpr_synthetics_only.reset_index(drop=True)\n",
    "data_tpr_fpr_synthetics_only[\"type\"] = \"synthetics_only\"\n",
    "\n",
    "data_auc_synthetics = pd.DataFrame()\n",
    "\n",
    "for idx in range(len(artifacts[\"training_auc\"])):\n",
    "    for nuclide in artifacts[\"training_auc\"][idx].keys():\n",
    "        nuclide_df = pd.DataFrame([artifacts[\"training_auc\"][idx][nuclide]], columns=[\"training_auc\"])\n",
    "        nuclide_df[\"validation_auc\"] = artifacts[\"validation_auc\"][idx][nuclide]\n",
    "        nuclide_df[\"nuclide\"] = nuclide\n",
    "        nuclide_df[\"epoch\"] = idx\n",
    "        data_auc_synthetics = pd.concat([data_auc_synthetics, nuclide_df], axis=0)\n",
    "\n",
    "data_auc_synthetics[\"type\"] = \"synthetics_only\"\n",
    "data_auc_synthetics = data_auc_synthetics.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tpr_fpr_all = pd.concat([data_tpr_fpr, data_tpr_fpr_synthetics_only], axis=0).reset_index(drop=True)\n",
    "data_tpr_fpr_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_auc_all = pd.concat([data_auc_synthetics, data_auc], axis=0).reset_index(drop=True)\n",
    "data_auc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['text.usetex'] = True\n",
    "\n",
    "def create_plot_axis(ax, data):\n",
    "    training_min_loss_x = data[\"training_macro_loss\"].max()\n",
    "    training_min_loss_y = data.loc[data[\"training_macro_loss\"] == training_min_loss_x][\"epoch\"].values[0]\n",
    "\n",
    "    validation_min_loss_x = data[\"validation_macro_loss\"].max()\n",
    "    validation_min_loss_y = data.loc[data[\"validation_macro_loss\"] == validation_min_loss_x][\"epoch\"].values[0]\n",
    "\n",
    "    ax.plot(data[\"epoch\"], data[\"training_macro_loss\"], color=\"black\",\n",
    "             label=\"training_macro_loss\")\n",
    "\n",
    "    ax.plot(data[\"epoch\"], data[\"validation_macro_loss\"], color=\"red\",\n",
    "             label=\"validation_macro_loss\")\n",
    "    ax.annotate(f'MAX(Macro-AUC) Training = {round(training_min_loss_x, 2)}',\n",
    "                 ha='center', va='bottom',\n",
    "                 size='large',\n",
    "                 xytext=(training_min_loss_y, 1), xy=(training_min_loss_y, training_min_loss_x), arrowprops={'facecolor': 'darkgrey'}, alpha=0.5)\n",
    "\n",
    "    ax.annotate(f'MAX(Macro-AUC) Validierung = {round(validation_min_loss_x, 2)}',\n",
    "                 ha='center', va='bottom',\n",
    "                 size='large',\n",
    "                 xytext=(validation_min_loss_y, 1), xy=(validation_min_loss_y, validation_min_loss_x), arrowprops={'facecolor': 'darkgrey'}, alpha=0.5)\n",
    "    ax.set_ylim(0,1)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(ncols=1)\n",
    "create_plot_axis(axs, data_mic_mac_loss_all.loc[data_mic_mac_loss_all[\"type\"] == \"synthetics_only\"].reset_index(drop=True))\n",
    "create_plot_axis(axs, data_mic_mac_loss_all.loc[data_mic_mac_loss_all[\"type\"] == \"measurements_only\"].reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_auc_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def format_isotope_label(row, auc_df):\n",
    "    match = re.match(r\"([a-zA-Z]+)(\\d+)\", row[\"nuclide\"])\n",
    "    if match:\n",
    "        element, mass = match.groups()\n",
    "        if auc_df is not None:\n",
    "            val_auc = auc_df.loc[auc_df[\"nuclide\"] == row[\"nuclide\"], \"validation_auc\"].values[0]\n",
    "            train_auc = auc_df.loc[auc_df[\"nuclide\"] == row[\"nuclide\"], \"training_auc\"].values[0]\n",
    "        # $\\nAUC-Training={round(train_auc,2)}\\nAUC-Validation={round(val_auc,2)}\n",
    "        return f\"$^{{{mass}}}{element.capitalize()}$\"\n",
    "    return row[\"nuclide\"]\n",
    "\n",
    "# Prepare one filtered DataFrame\n",
    "def prepare_data(type_name):\n",
    "    loss_df = data_mic_mac_loss_all[data_mic_mac_loss_all[\"type\"] == type_name]\n",
    "    max_val_loss = loss_df[\"validation_macro_loss\"].max()\n",
    "    epoch_best = loss_df.loc[loss_df[\"validation_macro_loss\"] == max_val_loss, \"epoch\"].values[0]\n",
    "\n",
    "    auc_df = data_auc_all[(data_auc_all[\"epoch\"] == epoch_best) & (data_auc_all[\"type\"] == type_name)]\n",
    "    df = data_tpr_fpr_all[(data_tpr_fpr_all[\"epoch\"] == epoch_best) & (data_tpr_fpr_all[\"type\"] == type_name)].copy()\n",
    "    df[\"nuclide\"] = df.apply(lambda row: format_isotope_label(row, auc_df), axis=1)\n",
    "    return df, auc_df\n",
    "\n",
    "# Combine both types\n",
    "df_syn, auc_df_syn = prepare_data(\"synthetics_only\")\n",
    "df_mes, auc_df_mes = prepare_data(\"measurements_only\")\n",
    "combined_df = pd.concat([df_syn, df_mes])\n",
    "combined_df[\"type\"] = combined_df[\"type\"].map({\n",
    "    \"synthetics_only\": \"Synthetische Daten (1022 Datensätze)\",\n",
    "    \"measurements_only\": \"Gemessene Daten (1022 Datensätze)\"\n",
    "})\n",
    "combined_df_auc = pd.concat([auc_df_syn, auc_df_mes])\n",
    "\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "# Plot with relplot\n",
    "g = sns.relplot(\n",
    "    data=combined_df,\n",
    "    x=\"validation_fpr\",\n",
    "    y=\"validation_tpr\",\n",
    "    hue=\"nuclide\",\n",
    "    col=\"type\",\n",
    "    kind=\"line\",\n",
    "    drawstyle=\"steps-pre\",\n",
    "    facet_kws={\"sharex\": True, \"sharey\": True},\n",
    "    height=5,\n",
    "    aspect=1.2,\n",
    "    palette=\"tab10\"\n",
    ")\n",
    "\n",
    "titles = [\"(A) Synthetische Daten (1022 Datensätze)\" , \"(B) Gemessene Daten (1022 Datensätze)\"]\n",
    "itrs = 0\n",
    "for ax in g.axes.flat:\n",
    "    ax.plot([0, 1], [0, 1], ls=\"--\", color=\"black\", alpha=0.5, lw=2, zorder=100)\n",
    "    ax.set_xlabel(\"False Positive Rate (FPR)\", fontsize=12)\n",
    "    ax.set_ylabel(\"True Positive Rate (TPR)\", fontsize=12)\n",
    "    ax.set_title(titles[itrs], fontsize=14)\n",
    "    ax.grid(False)\n",
    "    itrs += 1\n",
    "\n",
    "sns.move_legend(g,\n",
    "                loc=\"lower center\",\n",
    "                bbox_to_anchor=(0.5, 1.02),\n",
    "                borderaxespad=0,\n",
    "                title=\"\",\n",
    "                ncol=6,\n",
    "                fontsize=14,\n",
    "                frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_auc.groupby(\"type\").mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['text.usetex'] = True\n",
    "combined_df_auc[\"validation_auc\"] = combined_df_auc[\"validation_auc\"].round(2)\n",
    "combined_df_auc[\"nuclide\"] = combined_df_auc.apply(lambda row: format_isotope_label(row, None), axis=1)\n",
    "\n",
    "combined_df_auc[\"type\"] = combined_df_auc[\"type\"].str.replace(\"measurements_only\", \"Gemessen\").str.replace(\"synthetics_only\", \"Synthetisch\")\n",
    "means = combined_df_auc.groupby(\"type\")[\"validation_auc\"].mean()\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 4))\n",
    "ax = sns.barplot(\n",
    "    combined_df_auc,\n",
    "    x=\"nuclide\",\n",
    "    y=\"validation_auc\",\n",
    "    hue=\"type\",\n",
    "    dodge=True,\n",
    "    palette=[sns.color_palette(\"Greys\")[-5], sns.color_palette(\"Greys\")[-3]],\n",
    "    legend=False,\n",
    ")\n",
    "\n",
    "colots = [sns.color_palette(\"Greys\")[-1], sns.color_palette(\"Greys\")[-2]]\n",
    "for idx, (label, mean_val) in enumerate(means.items()):\n",
    "    ax.axhline(y=mean_val, color=colots[idx], linestyle=\"--\", linewidth=0.5)\n",
    "    ax.text(\n",
    "        x=len(combined_df_auc[\"nuclide\"].unique()) - 0.3,  # near the right edge\n",
    "        y=mean_val - 0.02,\n",
    "        s=f\"$AUC_{{makro}}$ {label} = {mean_val:.2f}\",\n",
    "        color=colots[idx],\n",
    "        fontsize=10,\n",
    "        va=\"bottom\"\n",
    "    )\n",
    "\n",
    "ax.bar_label(ax.containers[0], fontsize=10)\n",
    "ax.bar_label(ax.containers[1], fontsize=10)\n",
    "\n",
    "plt.xlabel(\"Nuklid\", size=14, labelpad=10)\n",
    "plt.ylabel(\"Anzahl\", size=14, labelpad=10)\n",
    "plt.tick_params(axis='x', labelsize=12, bottom=True, pad=10)\n",
    "plt.tick_params(axis='y', labelsize=12, left=True)\n",
    "plt.grid(False)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.set_xlim(-0.5,10.5)\n",
    "\n",
    "fig.legend(labels=['AUC Validierung (synthetisch 1022 Datensätz)','AUC Validierung (gemessen 1022 Datensätze)'],\n",
    "            bbox_to_anchor=(0.7, 1.02),\n",
    "            borderaxespad=0,\n",
    "            ncol=3,\n",
    "            frameon=False\n",
    "           )\n",
    "\n",
    "\n",
    "# for line in leg.get_lines():\n",
    "#     line.set_linewidth(5)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
